{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#not needed right now (maybe ever?) doing one-hot encoding next cell\n",
    "#might have to use if we are doing the neutral thing\n",
    "s = \"\"\"\n",
    "#encodes a column of parties into -1, 0, or 1\n",
    "def encode_party(party_series):\n",
    "    types = {\"Democrat\": 0, \"Neutral\": 0, \"Republican\": 0}\n",
    "    output_array = []\n",
    "    for party in party_series:\n",
    "        if party == \"Democrat\":\n",
    "            types[\"Democrat\"] += 1\n",
    "        elif party == \"Neutral\":\n",
    "            types[\"Neutral\"] += 1\n",
    "        elif party == \"Republican\":\n",
    "            types[\"Republican\"] += 1\n",
    "            \n",
    "    resulting_encoding = []\n",
    "\n",
    "    for type_ in types.keys():\n",
    "        resulting_encoding.append(type_)\n",
    "    resulting_encoding.sort()\n",
    "    return np.asarray(resulting_encoding)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hydrating data\n",
    "tweets = pd.read_csv(\"data/ExtractedTweets.csv\")\n",
    "\n",
    "#Preprocessing data\n",
    "\n",
    "#one-hot encoding republican and democrat\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot = OneHotEncoder(dtype=np.int, sparse=True, categories='auto')\n",
    "parties = pd.DataFrame(onehot.fit_transform(tweets[[\"Party\"]])\\\n",
    "                            .toarray(),\n",
    "                            columns=[\"Democrat\", \"Republican\"]) #todo add neutral column & training data\n",
    "parties[\"Handle\"] = tweets.Handle\n",
    "tweets = tweets.drop([\"Party\"], axis=1)\n",
    "encoded_data = pd.merge(parties, tweets)\n",
    "\n",
    "corpus = []\n",
    "#Concatinates all tweets in a user's recent tweets, and tokenizes the result\n",
    "def concatinate_tweets(tweet_object):\n",
    "    document = \" \"\n",
    "    for tweets in tweet_object:\n",
    "        for single_tweet in tweets.values[0:200]: \n",
    "            if not single_tweet.startswith(\"RT\"):\n",
    "                corpus.append(single_tweet)\n",
    "                document += single_tweet + \" \"\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(document)\n",
    "\n",
    "\n",
    "#dictionary containing the user's handle and a string representation of 200 of their tweets\n",
    "handles_and_tweets = {}\n",
    "for handle in encoded_data.groupby([\"Handle\"])[\"Tweet\"]:\n",
    "    handles_and_tweets[handle[0]] = concatinate_tweets(handle[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Organizing the data for training here\n",
    "df = pd.DataFrame.from_dict(handles_and_tweets, orient='index')\n",
    "df = df.reset_index()\n",
    "df.rename(columns={'index':'Handle'}, inplace=True)\n",
    "finished_product = pd.merge(parties, df, how='right')\n",
    "finished_product = finished_product.set_index('Handle')\n",
    "finished_product = finished_product.drop_duplicates()\n",
    "X = finished_product.iloc[0:, 2:]\n",
    "Y = finished_product['Democrat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>109763</th>\n",
       "      <th>109764</th>\n",
       "      <th>109765</th>\n",
       "      <th>109766</th>\n",
       "      <th>109767</th>\n",
       "      <th>109768</th>\n",
       "      <th>109769</th>\n",
       "      <th>109770</th>\n",
       "      <th>109771</th>\n",
       "      <th>109772</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010251</td>\n",
       "      <td>0.065042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.010073</td>\n",
       "      <td>0.023968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>0.027785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>433 rows Ã— 109773 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1       2       3       4       5       6       7       \\\n",
       "0    0.000000  0.015093     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1    0.000000  0.006853     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2    0.000000  0.000000     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3    0.010251  0.065042     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4    0.000000  0.007036     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "..        ...       ...     ...     ...     ...     ...     ...     ...   \n",
       "428  0.000000  0.000000     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "429  0.000000  0.007746     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "430  0.010073  0.023968     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "431  0.007006  0.027785     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "432  0.000000  0.007140     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "     8       9       ...  109763  109764  109765  109766  109767  109768  \\\n",
       "0       0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1       0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2       0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3       0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4       0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "..      ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "428     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "429     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "430     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "431     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "432     0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "     109769  109770  109771  109772  \n",
       "0       0.0     0.0     0.0     0.0  \n",
       "1       0.0     0.0     0.0     0.0  \n",
       "2       0.0     0.0     0.0     0.0  \n",
       "3       0.0     0.0     0.0     0.0  \n",
       "4       0.0     0.0     0.0     0.0  \n",
       "..      ...     ...     ...     ...  \n",
       "428     0.0     0.0     0.0     0.0  \n",
       "429     0.0     0.0     0.0     0.0  \n",
       "430     0.0     0.0     0.0     0.0  \n",
       "431     0.0     0.0     0.0     0.0  \n",
       "432     0.0     0.0     0.0     0.0  \n",
       "\n",
       "[433 rows x 109773 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf-idf vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer=TfidfVectorizer()\n",
    "vectors=vectorizer.fit_transform(corpus)\n",
    "\n",
    "users_tweet_history = pd.Series(X.fillna('').values.tolist()).str.join(' ')\n",
    "\n",
    "vectorized_tweet_history = vectorizer.transform(users_tweet_history)\n",
    "\n",
    "tfidf_vectors = pd.DataFrame(vectorized_tweet_history.toarray())\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-af9963753ed3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# W2V encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# W2V encoding\n",
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec vectorization\n",
    "user_and_vectorized_tweets_word2vec = {}\n",
    "k = 0\n",
    "for index, tweets in X.iterrows(): #todo maybe not all tweets are same length - might just be getting max and assigning that to all of them\n",
    "    total_embeddings = np.zeros(300) # this number can be tweaked? maybe im not sure\n",
    "    vectorized_tweet = []\n",
    "    k+=1\n",
    "    for word in tweets:\n",
    "        try:\n",
    "            encoding_of_word = word_vectors.get_vector(word)\n",
    "        except KeyError:\n",
    "            encoding_of_word = np.zeros(300)\n",
    "        for i, embedding in enumerate(encoding_of_word):\n",
    "            total_embeddings[i] = total_embeddings[i] + embedding\n",
    "    for i in range(len(total_embeddings)):\n",
    "        total_embeddings[i] = total_embeddings[i] / len(tweets)\n",
    "        \n",
    "    user_and_vectorized_tweets_word2vec[index] = total_embeddings\n",
    "    print(f'finished row {k}/433')\n",
    "    \n",
    "handle_and_vector_word2vec = pd.DataFrame.from_dict(user_and_vectorized_tweets_word2vec, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting up the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#tfidf split\n",
    "X_train_tfidf, X_test_tfidf, Y_train_tfidf, Y_test_tfidf = train_test_split(tfidf_vectors, Y, test_size = 0.2)\n",
    "\n",
    "#Word2vec split\n",
    "X_train_w2v, X_test_w2v, Y_train_w2v, Y_test_w2v = train_test_split(handle_and_vector_word2vec, Y, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on model - MLP classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#Grid search CV\n",
    "parameters = {'solver': ['lbfgs'],\n",
    "              'max_iter': [1000,1500,2000 ],\n",
    "              'alpha': 10.0 ** -np.arange(1, 10),\n",
    "              'hidden_layer_sizes':np.arange(10, 15)}\n",
    "\n",
    "tuned_MLP_classifier_w2v = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "tuned_MLP_classifier_tfidf = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "\n",
    "\n",
    "#Word2Vec evaluation\n",
    "tuned_MLP_classifier_w2v.fit(X_train_w2v, Y_train_w2v)\n",
    "w2v_score = tuned_MLP_classifier_w2v.score(X_test_w2v.values, Y_test_w2v)\n",
    "w2v_best_params = tuned_MLP_classifier_w2v.best_params_\n",
    "print(f'Word2vec score and best params:\\n{w2v_score}\\n{w2v_best_params}')\n",
    "\n",
    "#tfidf evaluation\n",
    "tuned_MLP_classifier_tfidf.fit(X_train_tfidf, Y_train_tfidf)\n",
    "tfidf_score = tuned_MLP_classifier_tfidf.score(X_test_tfidf.values, Y_test_tfidf)\n",
    "tfidf_best_params = tuned_MLP_classifier_tfidf.best_params_\n",
    "print(f'TfIdf score and best params:\\n{tfidf_score}\\n{tfidf_best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation for MLP Classifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_predictions_w2v = tuned_MLP_classifier_w2v.predict(X_test_w2v)\n",
    "print(\"Word2Vec Confusion matrix and accuracy:\")\n",
    "print(confusion_matrix(Y_test_w2v.astype(int), y_predictions_w2v.astype(int)))\n",
    "print(accuracy_score(Y_test_w2v.astype(int), y_predictions_w2v.astype(int)))\n",
    "\n",
    "y_predictions_tfidf = tuned_MLP_classifier_tfidf.predict(X_test_tfidf)\n",
    "print(\"\\nTfIdf Confusion matrix and accuracy:\")\n",
    "print(confusion_matrix(Y_test_tfidf.astype(int), y_predictions_tfidf.astype(int)))\n",
    "print(accuracy_score(Y_test_tfidf.astype(int), y_predictions_tfidf.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on model - Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Grid search for hyperparameter tuning (for random forest)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 5)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 150, num = 5)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint.pprint(random_grid)\n",
    "tuned_RF_classifier_w2v = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid,\n",
    "                                  n_iter = 100, cv = 3, verbose=2, random_state=42,\n",
    "                                  n_jobs = -1)\n",
    "tuned_RF_classifier_tfidf = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid,\n",
    "                                  n_iter = 100, cv = 3, verbose=2, random_state=42,\n",
    "                                  n_jobs = -1)\n",
    "tuned_RF_classifier_w2v.fit(X_train_w2v, Y_train_w2v)\n",
    "tuned_RF_classifier_tfidf.fit(X_train_tfidf, Y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation for RandomForest Classifier\n",
    "\n",
    "#Evaluation:\n",
    "print(\"Word2Vec results:\")\n",
    "print(f'Training Score: {tuned_RF_classifier_w2v.score(X_train_w2v, Y_train_w2v)} '\n",
    "      + f'\\n Validation Score: {tuned_RF_classifier_w2v.score(X_test_w2v, Y_test_w2v)}')\n",
    "\n",
    "#Evaluation:\n",
    "print(\"TfIdf results:\")\n",
    "print(f'Training Score: {tuned_RF_classifier_tfidf.score(X_train_tfidf, Y_train_tfidf)} '\n",
    "      + f'\\n Validation Score: {tuned_RF_classifier_tfidf.score(X_test_tfidf, Y_test_tfidf)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare to data on Congressional Meetings Speech\n",
    "import glob\n",
    "\n",
    "passages = []\n",
    "labels = []\n",
    "for file in glob.glob(\"./data/congressional_hearings_data/\" +\"*D*.txt\"):\n",
    "    inputfile = open(file)\n",
    "    a = inputfile.readline()\n",
    "    passages.append(a)\n",
    "    labels.append(1)\n",
    "    inputfile.close()\n",
    "\n",
    "for file in glob.glob(\"./data/congressional_hearings_data/\" +\"*R*.txt\"):\n",
    "    inputfile = open(file)\n",
    "    a = inputfile.readline()\n",
    "    passages.append(a)\n",
    "    labels.append(0)\n",
    "    inputfile.close()\n",
    "\n",
    "\n",
    "#vectorize sents with tfidf\n",
    "passages_vectorized = vectorizer.transform(passages)\n",
    "passages_tfidf = pd.DataFrame(passages_vectorized.toarray())\n",
    "\n",
    "#vectorize sents with word2vec\n",
    "word2vec_passages = []\n",
    "for passage in passages:\n",
    "    word_list = passage.split(\" \")\n",
    "    for word in word_list:\n",
    "        total_embeddings = np.zeros(300)\n",
    "        try:\n",
    "            encoding_of_word = word_vectors.get_vector(word)\n",
    "        except KeyError:\n",
    "            encoding_of_word = np.zeros(300)\n",
    "        for i, embedding in enumerate(encoding_of_word):\n",
    "            total_embeddings[i] = total_embeddings[i] + embedding\n",
    "    for i in range(len(total_embeddings)):\n",
    "        total_embeddings[i] = total_embeddings[i] / len(word_list)\n",
    "        \n",
    "    word2vec_passages.append(total_embeddings)\n",
    "\n",
    "#Evaluate on models\n",
    "#TFIDF - MLP\n",
    "print(\"TFIDF results:\")\n",
    "passages_tfidf_mlp = tuned_MLP_classifier_tfidf.score(passages_tfidf.values, labels)\n",
    "print(f'\\tMLP score of passages is: {passages_tfidf_mlp}')\n",
    "\n",
    "#TFIDF - RF\n",
    "passages_tfidf_rf = tuned_RF_classifier_tfidf.score(passages_tfidf.values, labels)\n",
    "print(f'\\tRF score of passages is: {passages_tfidf_rf}')\n",
    "\n",
    "#W2V - MLP\n",
    "print(\"Word2Vec results:\")\n",
    "passages_w2v_mlp = tuned_MLP_classifier_w2v.score(word2vec_passages, labels)\n",
    "print(f'\\tMLP Score of passages is: {passages_w2v_mlp}')\n",
    "\n",
    "#W2V - RF\n",
    "passages_w2v_rf = tuned_RF_classifier_w2v.score(word2vec_passages, labels)\n",
    "print(f'\\tRF score of passages is: {passages_w2v_rf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to FB data\n",
    "\n",
    "# populate df \n",
    "fb_df = pd.read_csv(\"data/FacebookPosts.csv\")\n",
    "\n",
    "# one-hot encoding the parties\n",
    "fb_parties = pd.DataFrame(onehot.fit_transform(fb_df[[\"Party\"]]).toarray(), columns=[\"Democrat\", \"Republican\"])\n",
    "fb_parties[\"Id\"] = fb_df.Id\n",
    "fb_df = fb_df.drop([\"Party\"], axis=1)\n",
    "fb_df = pd.merge(fb_parties, fb_df)\n",
    "\n",
    "# concatenating posts and grouping them by Id\n",
    "group = fb_df.groupby(['Id'])\n",
    "list(group['Text'])\n",
    "\n",
    "def concatenate_posts(posts_object):\n",
    "    document = \" \"\n",
    "    for posts in posts_object:\n",
    "        for post in posts.values[0:200]: \n",
    "            document += post + \" \"\n",
    "    # just using the tweet tokenizer because it should tokenize fb posts just fine\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(document)\n",
    "\n",
    "ids_and_posts = {}\n",
    "for i in encoded_fb_data.groupby([\"Id\"])[\"Text\"]:\n",
    "    ids_and_posts[i[0]] = concatenate_posts(list(i[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizing the data for testing\n",
    "#Organizing the data for training here\n",
    "ids_and_posts_df = pd.DataFrame.from_dict(ids_and_posts, orient='index')\n",
    "ids_and_posts_df = ids_and_posts_df.reset_index()\n",
    "ids_and_posts_df.rename(columns={'index':'Id'}, inplace=True)\n",
    "finished_fb_df = pd.merge(fb_parties, ids_and_posts_df)\n",
    "finished_fb_df = finished_fb_df.set_index('Id')\n",
    "finished_fb_df = finished_fb_df.drop_duplicates()\n",
    "X = finished_fb_df.iloc[0:, 2:]\n",
    "Y = finished_fb_df['Democrat']\n",
    "\n",
    "# i think this is right so far but for some reason i had to rerun everything and now it's saying that\n",
    "# modules cant be found so idk what to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
